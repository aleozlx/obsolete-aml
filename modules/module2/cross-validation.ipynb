{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [obsolete] Cross Validation\n",
    "\n",
    "Datasets:\n",
    "\n",
    "1. [Credit Card Fraud Detection](https://www.kaggle.com/dalpozz/creditcardfraud)\n",
    "\n",
    "Dataset download:\n",
    "\n",
    "1. https://www.kaggle.com/dalpozz/creditcardfraud/downloads/creditcardfraud.zip\n",
    "\n",
    "References:\n",
    "\n",
    "+ [Cross-validation @ scikit-learn](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "+ https://www.kaggle.com/arathee2/achieving-100-accuracy\n",
    "+ https://www.kaggle.com/edunuke/binary-decision-tree-with-cross-validation/notebook\n",
    "+ [Why every statistician should know about cross-validation](https://robjhyndman.com/hyndsight/crossvalidation/)\n",
    "\n",
    "More:\n",
    "\n",
    "+ https://www.kaggle.com/maximilianhahn/manager-skill-for-cross-validation-pipelines\n",
    "+ https://www.kaggle.com/alexandrebarachant/simple-grasp-cross-validation/code\n",
    "+ https://www.kaggle.com/solomonk/proper-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "DATASET = lambda fname: os.path.join('datasets', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Check dataset dimension and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (284807, 31) distribution {0: 284315, 1: 492}\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(DATASET('creditcard.csv'))\n",
    "dataset_original = pd.read_csv(DATASET('creditcard.csv'))\n",
    "\n",
    "def brief_dataset(ds):\n",
    "    \"\"\" Print a brief report of the dataset \"\"\"\n",
    "    print('shape', ds.shape,\n",
    "          'distribution', { # a mapping from class to class subset shape\n",
    "              cls:ds.loc[lambda i: i.Class == cls, :].shape[0] # filter by class\n",
    "                  for cls in ds.Class.unique() # uniquely enumerate all classes\n",
    "          })\n",
    "    \n",
    "brief_dataset(dataset_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first a few rows for a preview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_original[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample for training\n",
    "\n",
    "Resample dataset to get more balanced class distribution.\n",
    "\n",
    "API ref: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (8000, 31) distribution {0: 4000, 1: 4000}\n"
     ]
    }
   ],
   "source": [
    "NORMAL = lambda i: i.Class == 0\n",
    "FRAUD = lambda i: i.Class == 1\n",
    "\n",
    "subset_normal = dataset_original.loc[NORMAL, :].sample(n = 4000) # downsample\n",
    "subset_fraud = dataset_original.loc[FRAUD, :].sample(n = 4000, replace = True) # upsample\n",
    "dataset_resampled = pd.concat([subset_normal, subset_fraud]).reset_index(drop=True)\n",
    "brief_dataset(dataset_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Training\n",
    "\n",
    "API ref:\n",
    "\n",
    "+ [Indexing @ pandas](http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing)\n",
    "+ [SVM API @ scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: [(5600, 30), (2400, 30), (5600,), (2400,)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99624999999999997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    dataset_resampled.iloc[:,:-1], # features\n",
    "    dataset_resampled.Class, # labels\n",
    "    test_size=0.3 # test dataset fraction\n",
    ")\n",
    "\n",
    "print('shapes:', [i.shape for i in (X_train, X_test, y_train, y_test)])\n",
    "\n",
    "classifier = svm.SVC(kernel='rbf', C=1, gamma=0.1).fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computing cross-validated metrics\n",
    "\n",
    "Create another resample for cross-validation. (Only because 280000+ rows takes too long, cross-validation here isn't running on the original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_cv = dataset_original.sample(n = 5000).reset_index(drop=True)\n",
    "# sklearn.model_selection.cross_val_score(\n",
    "#     classifier,\n",
    "#     dataset_cv.iloc[:,:-1], # features\n",
    "#     dataset_cv.Class, # labels\n",
    "#     cv=10 # 10-fold cross validation\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Divide normal and abnormal datasets into folds\n",
    "2. Recombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal folds [28432, 28432, 28432, 28432, 28432, 28431, 28431, 28431, 28431, 28431]\n",
      "Fraud folds [50, 50, 49, 49, 49, 49, 49, 49, 49, 49]\n",
      "0 X_train (256325, 30) X_test (28482, 30) y_train (256325,) y_test (28482,)\n",
      "1 X_train (256325, 30) X_test (28482, 30) y_train (256325,) y_test (28482,)\n",
      "2 X_train (256326, 30) X_test (28481, 30) y_train (256326,) y_test (28481,)\n",
      "3 X_train (256326, 30) X_test (28481, 30) y_train (256326,) y_test (28481,)\n",
      "4 X_train (256326, 30) X_test (28481, 30) y_train (256326,) y_test (28481,)\n",
      "5 X_train (256327, 30) X_test (28480, 30) y_train (256327,) y_test (28480,)\n",
      "6 X_train (256327, 30) X_test (28480, 30) y_train (256327,) y_test (28480,)\n",
      "7 X_train (256327, 30) X_test (28480, 30) y_train (256327,) y_test (28480,)\n",
      "8 X_train (256327, 30) X_test (28480, 30) y_train (256327,) y_test (28480,)\n",
      "9 X_train (256327, 30) X_test (28480, 30) y_train (256327,) y_test (28480,)\n"
     ]
    }
   ],
   "source": [
    "N_FOLDS = 10\n",
    "# Shuffle\n",
    "dataset_cv = dataset_original.sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "# Divide folds\n",
    "folds_normal = np.array_split(dataset_cv.loc[NORMAL, :], N_FOLDS)\n",
    "folds_fraud = np.array_split(dataset_cv.loc[FRAUD, :], N_FOLDS)\n",
    "print('Normal folds', [i.shape[0] for i in folds_normal])\n",
    "print('Fraud folds', [i.shape[0] for i in folds_fraud]) \n",
    "\n",
    "# Cross validation\n",
    "for i in range(N_FOLDS):\n",
    "    # Split training/test datasets\n",
    "    train = np.concatenate([np.concatenate([folds_normal[j], folds_fraud[j]]) for j in range(N_FOLDS) if j!=i])\n",
    "    test = np.concatenate([folds_normal[i], folds_fraud[i]])\n",
    "    X_train = train[:,:-1]\n",
    "    X_test = test[:,:-1]\n",
    "    y_train = train[:,-1]\n",
    "    y_test = test[:,-1]\n",
    "    print(i, 'X_train', X_train.shape, 'X_test', X_test.shape, 'y_train', y_train.shape, 'y_test', y_test.shape)\n",
    "    \n",
    "    # classifier = svm.SVC(kernel='rbf', C=1, gamma=0.1).fit(X_train, y_train)\n",
    "    # classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
